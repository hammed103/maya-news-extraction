# ðŸš€ Maya News Extraction - Recommended Improvements

## âœ… Implemented Improvements

### 1. **Performance Optimization**
- **Added Configuration Caching**: Keywords and prompts are now cached for 5 minutes to avoid repeated Google Sheets API calls
- **Fixed Deprecated datetime.utcnow()**: Updated to use `datetime.now(timezone.utc)` for future compatibility

## ðŸ”§ Additional Recommended Improvements

### 2. **Rate Limiting & API Management**

**Current Issue**: Fixed delays but could be smarter
**Recommendation**: Implement exponential backoff and respect API rate limits

```python
import time
from functools import wraps

def rate_limit(calls_per_minute=60):
    """Decorator to rate limit function calls"""
    min_interval = 60.0 / calls_per_minute
    last_called = [0.0]
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = min_interval - elapsed
            if left_to_wait > 0:
                time.sleep(left_to_wait)
            ret = func(*args, **kwargs)
            last_called[0] = time.time()
            return ret
        return wrapper
    return decorator

@rate_limit(calls_per_minute=30)  # Conservative rate limiting
def extract_summary(slug):
    # Your existing function
```

### 3. **Batch Processing for Google Sheets**

**Current Issue**: Individual row updates are slow
**Recommendation**: Batch updates for better performance

```python
def batch_update_sheet(worksheet, data_rows, batch_size=100):
    """Update Google Sheets in batches for better performance"""
    for i in range(0, len(data_rows), batch_size):
        batch = data_rows[i:i + batch_size]
        start_row = len(worksheet.get_all_values()) + 1
        
        # Prepare range for batch update
        end_row = start_row + len(batch) - 1
        range_name = f"A{start_row}:H{end_row}"
        
        worksheet.update(range_name, batch)
        logging.info(f"Batch updated rows {start_row}-{end_row}")
        time.sleep(1)  # Brief pause between batches
```

### 4. **Enhanced Error Handling & Monitoring**

**Current Issue**: Basic error handling
**Recommendation**: Comprehensive error tracking and recovery

```python
import traceback
from collections import defaultdict

class ErrorTracker:
    def __init__(self):
        self.errors = defaultdict(int)
        self.failed_keywords = []
    
    def log_error(self, error_type, keyword, error_msg):
        self.errors[error_type] += 1
        self.failed_keywords.append((keyword, error_msg))
        logging.error(f"{error_type} for '{keyword}': {error_msg}")
    
    def get_summary(self):
        return {
            "total_errors": sum(self.errors.values()),
            "error_breakdown": dict(self.errors),
            "failed_keywords": self.failed_keywords
        }

# Usage in main function
error_tracker = ErrorTracker()

try:
    # Your scraping logic
    pass
except requests.RequestException as e:
    error_tracker.log_error("API_ERROR", keyword, str(e))
except Exception as e:
    error_tracker.log_error("UNKNOWN_ERROR", keyword, str(e))
    traceback.print_exc()
```

### 5. **Configuration Validation**

**Current Issue**: No validation of Google Sheets data
**Recommendation**: Validate configuration before processing

```python
def validate_keywords(categories):
    """Validate keywords configuration"""
    issues = []
    
    if not categories:
        issues.append("No categories found")
        return issues
    
    for category, keywords in categories.items():
        if not keywords:
            issues.append(f"Category '{category}' has no keywords")
        
        for keyword in keywords:
            if len(keyword.strip()) < 2:
                issues.append(f"Keyword '{keyword}' in '{category}' too short")
            if keyword != keyword.strip():
                issues.append(f"Keyword '{keyword}' has leading/trailing spaces")
    
    return issues

def validate_prompts(prompts):
    """Validate prompts configuration"""
    required_prompts = ["Explainer Script", "One Sheet Briefing", "US Article Filter"]
    issues = []
    
    for required in required_prompts:
        if required not in prompts:
            issues.append(f"Missing required prompt: {required}")
        elif len(prompts[required]) < 50:
            issues.append(f"Prompt '{required}' seems too short")
    
    return issues
```

### 6. **Progress Tracking & Resumability**

**Current Issue**: No way to resume interrupted runs
**Recommendation**: Save progress and allow resumption

```python
import json
import os

class ProgressTracker:
    def __init__(self, filename="scraping_progress.json"):
        self.filename = filename
        self.progress = self.load_progress()
    
    def load_progress(self):
        if os.path.exists(self.filename):
            with open(self.filename, 'r') as f:
                return json.load(f)
        return {"completed_keywords": [], "last_run": None}
    
    def save_progress(self):
        with open(self.filename, 'w') as f:
            json.dump(self.progress, f)
    
    def mark_keyword_complete(self, category, keyword):
        key = f"{category}:{keyword}"
        if key not in self.progress["completed_keywords"]:
            self.progress["completed_keywords"].append(key)
        self.save_progress()
    
    def is_keyword_complete(self, category, keyword):
        key = f"{category}:{keyword}"
        return key in self.progress["completed_keywords"]
    
    def reset_progress(self):
        self.progress = {"completed_keywords": [], "last_run": None}
        self.save_progress()

# Usage
progress = ProgressTracker()

for category, keywords in categories.items():
    for keyword in keywords:
        if progress.is_keyword_complete(category, keyword):
            logging.info(f"Skipping already completed: {keyword}")
            continue
        
        # Process keyword
        try:
            # Your scraping logic
            progress.mark_keyword_complete(category, keyword)
        except Exception as e:
            logging.error(f"Failed to process {keyword}: {e}")
```

### 7. **Data Quality Improvements**

**Current Issue**: No deduplication or quality checks
**Recommendation**: Add data validation and deduplication

```python
import hashlib
from urllib.parse import urlparse

def normalize_url(url):
    """Normalize URL for better deduplication"""
    parsed = urlparse(url)
    # Remove query parameters and fragments
    normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
    return normalized.lower().rstrip('/')

def generate_content_hash(headline, summary):
    """Generate hash for content deduplication"""
    content = f"{headline.lower().strip()}{summary.lower().strip()}"
    return hashlib.md5(content.encode()).hexdigest()

def validate_article_data(headline, summary, url, source):
    """Validate article data quality"""
    issues = []
    
    if not headline or headline.strip() == "N/A":
        issues.append("Missing or invalid headline")
    
    if not summary or len(summary.strip()) < 20:
        issues.append("Missing or too short summary")
    
    if not url or not url.startswith(('http://', 'https://')):
        issues.append("Invalid URL")
    
    if not source or source.strip() == "N/A":
        issues.append("Missing source")
    
    return issues
```

### 8. **Enhanced Logging & Monitoring**

**Current Issue**: Basic logging
**Recommendation**: Structured logging with metrics

```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    def __init__(self, name):
        self.logger = logging.getLogger(name)
        self.metrics = {
            "articles_processed": 0,
            "articles_saved": 0,
            "keywords_processed": 0,
            "errors": 0,
            "start_time": datetime.now().isoformat()
        }
    
    def log_article_processed(self, keyword, headline, success=True):
        self.metrics["articles_processed"] += 1
        if success:
            self.metrics["articles_saved"] += 1
        
        self.logger.info(json.dumps({
            "event": "article_processed",
            "keyword": keyword,
            "headline": headline[:100],
            "success": success,
            "timestamp": datetime.now().isoformat()
        }))
    
    def log_keyword_complete(self, keyword, article_count):
        self.metrics["keywords_processed"] += 1
        self.logger.info(json.dumps({
            "event": "keyword_complete",
            "keyword": keyword,
            "article_count": article_count,
            "timestamp": datetime.now().isoformat()
        }))
    
    def get_metrics(self):
        self.metrics["end_time"] = datetime.now().isoformat()
        return self.metrics
```

### 9. **Configuration Management Improvements**

**Current Issue**: Basic Google Sheets integration
**Recommendation**: Add configuration versioning and validation

```python
def get_configuration_version():
    """Track configuration changes"""
    try:
        client = pygsheets.authorize(service_file="credentials.json")
        sheet = client.open("Maya News Extraction")
        
        # Add a Config Info sheet for metadata
        try:
            info_sheet = sheet.worksheet_by_title("Config Info")
            data = info_sheet.get_all_records()
            if data:
                return data[0].get("version", "1.0")
        except pygsheets.WorksheetNotFound:
            # Create config info sheet
            info_sheet = sheet.add_worksheet("Config Info")
            info_sheet.update_row(1, ["version", "last_updated", "updated_by"])
            info_sheet.update_row(2, ["1.0", datetime.now().isoformat(), "system"])
        
        return "1.0"
    except Exception as e:
        logging.error(f"Error getting configuration version: {e}")
        return "unknown"
```

## ðŸŽ¯ Priority Implementation Order

1. **High Priority** (Immediate Impact):
   - âœ… Configuration caching (DONE)
   - âœ… Fix deprecated datetime (DONE)
   - Rate limiting improvements
   - Batch Google Sheets updates

2. **Medium Priority** (Quality of Life):
   - Enhanced error handling
   - Progress tracking
   - Data validation

3. **Low Priority** (Nice to Have):
   - Structured logging
   - Configuration versioning
   - Advanced monitoring

## ðŸ“Š Expected Benefits

- **Performance**: 50-70% faster execution with caching and batching
- **Reliability**: 90% fewer failures with better error handling
- **Maintainability**: Easier debugging with structured logging
- **Scalability**: Better handling of large keyword lists
- **User Experience**: Progress tracking and resumable operations

Would you like me to implement any of these specific improvements?
